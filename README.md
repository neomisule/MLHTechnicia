# ROMA-VLM: Multimodal Vision Extension for ROMA

This project extends the ROMA (Recursive Open Meta-Agents) framework with Vision Language Model (VLM) capabilities, allowing each node (Atomizer, Planner, Executor, Aggregator, Verifier) to process both text and images.



# ğŸ¯ Features

- **VLM-Powered Nodes**: Every ROMA node can accept images as inputs
- **Different VLMs per Node**: Configure different vision models for each agent
- **Configurable Prediction Strategies**: Use Chain of Thought, ReAct, or CodeAct for each node
- **Signature Instructions**: Customize agent behavior with domain-specific prompts (NEW!)
- **Few-Shot Learning**: Add demos for improved vision analysis accuracy (NEW!)
- **Recursive Image Processing**: Images flow through the task hierarchy
- **Zero ROMA Modifications**: Uses ROMA as a library, no core changes needed
- **Drop-in Replacement**: Compatible with existing ROMA workflows

# ğŸš€ Installation

```bash
# Create venv
python3 -m venv vlm-roma-env
source vlm-roma-env/bin/activate  # This is for macOS/Linux but for windows use roma-vlm\Scripts\activate 
pip install -r requirements.txt # install all the dependences

# now we need to install ROMA into this environment and hence use following
cd ..
git clone https://github.com/sentient-agi/ROMA.git
cd ../ROMA
pip install -e .
cd ../MLHTechnicia
pip install -e . # downloading roma-vlm into our env so that we can import it anywhere easily
```

# ğŸ“¦ Project Structure

```
roma-vlm-extension/
â”œâ”€â”€ roma_vlm/
â”‚   â”œâ”€â”€ signatures/          # Multimodal signatures extending ROMA
â”‚   â”œâ”€â”€ modules/             # VLM-enabled modules (Atomizer, Planner, etc.)
â”‚   â”œâ”€â”€ engine/              # Multimodal solve() function
â”‚   â”œâ”€â”€ config/              # VLM-specific configuration
â”‚   â””â”€â”€ utils/               # Image handling utilities
â”œâ”€â”€ examples/                # Usage examples
â”œâ”€â”€ tests/                   # Test suite
â”œâ”€â”€ config/                  # YAML configuration profiles
â””â”€â”€ pyproject.toml
```

# ğŸ”„ Data Flow

## 1. Single Image Analysis (Atomic Task)

```
User Input: goal + image
         â†“
MultimodalAtomizer(goal, [image])
  â”œâ”€ VLM analyzes: "Can I handle this directly?"
  â””â”€ Result: is_atomic = True
         â†“
MultimodalExecutor(goal, [image])
  â”œâ”€ VLM analyzes image
  â””â”€ Result: analysis output
         â†“
MultimodalVerifier(goal, [image], output)
  â”œâ”€ VLM checks: "Does output match image?"
  â””â”€ Result: verdict = True
         â†“
Final Result
```

## 2. Multi-Image Task with Planning

```
User Input: goal + images
         â†“
MultimodalAtomizer(goal, [img1, img2, img3])
  â”œâ”€ VLM: "This needs decomposition"
  â””â”€ Result: is_atomic = False
         â†“
MultimodalPlanner(goal, [img1, img2, img3])
  â”œâ”€ VLM analyzes images
  â”œâ”€ Creates subtasks referencing specific images
  â””â”€ Result: [subtask1, subtask2, subtask3]
         â†“
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼         â–¼        â–¼
Executor   Executor  Executor
(img1)     (img2)    (img3)
    â”‚         â”‚        â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
MultimodalAggregator(goal, [img1,img2,img3], results)
  â”œâ”€ VLM synthesizes with image context
  â””â”€ Result: synthesized output
         â†“
MultimodalVerifier(goal, [img1,img2,img3], output)
  â””â”€ Result: verdict
         â†“
Final Result
```